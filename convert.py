#! /usr/bin/env python3


# paj-fetcher -- Download and convert data from PAJ
# By: Evgeniy Smirnov <rassouljb@gmail.com>
#
# Copyright (C) 2020 Cepremap
# https://git.nomics.world/dbnomics-fetchers/paj-fetcher
#
# paj-fetcher is free software; you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# paj-fetcher is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.


"""Convert data from PAJ to DBnomics data model
(see https://git.nomics.world/dbnomics/dbnomics-data-model/).

Usage:
    {self_filename} <source_dir> <target_dir> [options]

source_dir: path of source directory containing World Bank series generated by
            download.py script
target_dir: path of target directory to write datasets & series in DBnomics format
            => all files will be deleted

Options:
    --debug show debug output, and compute some tests that makes process slower

Read source data from a source directory, write converted data to a target directory.

See also `.gitlab-ci.yml` in which data is committed to a Git repository of converted
 data.
"""

import argparse
import json
import logging
import os
import re
import shutil
import sys
from pathlib import Path
from typing import Sequence

import xlrd
from dbnomics_fetcher_toolbox.arguments import add_arguments_for_convert
from dbnomics_fetcher_toolbox.logging_utils import setup_logging
from dbnomics_fetcher_toolbox.sdmx_v2_0 import DatasetStructure, Attribute, Concept, \
    CodeList, Code, AttachmentLevel, Dimension, Series, Value, Obs, \
    parse_observation_value, NAN, structure_to_dataset_json, series_to_series_json
from dbnomics_json_errors import ErrorsArtifact

PROVIDER_DATA = {
    "code": "paj",
    "name": "Petroleum Association of Japan",
    "region": "Japan",
    "terms_of_use": "",
    "website": "https://www.paj.gr.jp/english/statis/",
}

DATASETS_DEFINITIONS = {
    "01": DatasetStructure(
        attributes=[Attribute(concept_id="UNIT", codelist_id="CL_UNIT",
                              attachment_level=AttachmentLevel.SERIES)],
        codelists=[CodeList(id="CL_UNIT",
                            codes=[Code(value="kl", descriptions={"en": "kiloliter"})],
                            names={"en": "Unit of measure"}),
                   CodeList(id="CL_FREQ",
                            codes=[Code(value="M", descriptions={"en": "Monthly"})],
                            names={"en": "Frequency"}),
                   CodeList(id="CL_INDEX",
                            codes=[Code(value="P", descriptions={"en": "Production"}),
                                   Code(value="I", descriptions={"en": "Import"}),
                                   Code(value="NRU",
                                        descriptions={"en": "non-Refining use"}),
                                   Code(value="RT",
                                        descriptions={"en": "Refinery throughput"}),
                                   Code(value="BD", descriptions={"en": "(b/ï½„)"}),
                                   Code(value="RC",
                                        descriptions={"en": "Refining capacity"}),
                                   Code(value="U", descriptions={"en": "Utilization"}),
                                   Code(value="ES", descriptions={"en": "End stocks"})],
                            names={"en": "Measurement index"})
                   ],
        concepts=[Concept(id="UNIT", names={"en": "Unit of measure"}),
                  Concept(id="FREQ", names={"en": "Frequency"}),
                  Concept(id="INDEX", names={"en": "Index of measurements"})],
        dimensions=[Dimension(concept_id="FREQ", codelist_id="CL_FREQ"),
                    Dimension(concept_id="INDEX", codelist_id="CL_INDEX")],
        id="SDCO",
        names={"en": "Supply and Demand of Crude Oil"}
    )
}

REGEXS = {
    'period_monthly': r'(\d{4})\.(\d{2})',
}

log = logging.getLogger(__name__)


def main():
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('--log', default='WARNING', help='level of logging messages')
    add_arguments_for_convert(parser)
    args = parser.parse_args()

    setup_logging(args)

    source_dir = args.source_dir
    if not source_dir.exists():
        parser.error("Source input_dir {!r} not found".format(str(source_dir)))

    target_dir = args.target_dir
    if not target_dir.exists():
        parser.error("Target input_dir {!r} not found".format(str(target_dir)))

    nb_expected_datasets = 0
    errors_artifact = ErrorsArtifact()
    for dir, datasetStructure in DATASETS_DEFINITIONS.items():
        nb_expected_datasets += 1
        dataset_code = datasetStructure.id
        dataset_dir = target_dir / dataset_code
        try:
            convert_dataset(source_dir / dir, datasetStructure, dataset_dir)
        except Exception as e:
            if getattr(args, 'stop-on-exceptions', None):
                raise e
            log.warning("{!r} dataset aborted ! - {}".format(dataset_code, e))
            # Add error to artifacts
            errors_artifact.add_dataset_error(dataset_code, e)
            # Delete dataset input_dir
            shutil.rmtree(dataset_dir)
            continue

    # provider.json
    write_json_file(target_dir / 'provider.json', PROVIDER_DATA)

    return 0


def convert_dataset(input_dir: Path, structure: DatasetStructure, output_dir: Path):
    output_dir.mkdir(exist_ok=True, parents=True)
    dataset_source_xls = \
        sorted(input_dir.glob("*"), key=os.path.basename, reverse=True)[0]
    dataset_source = xlrd.open_workbook(dataset_source_xls, on_demand=True)
    if structure.id == 'SDCO':
        source_sheet = dataset_source.sheet_by_index(0)
        index_codes = structure.get_codelist('CL_INDEX').codes
        series_dict = {'M.' + c.value: Series(key=[
            Value(concept_id='FREQ', value='M'),
            Value(concept_id='INDEX', value=c.value),
        ], attributes=[Value(concept_id='UNIT', value='kl')], observations=[])
            for c in index_codes}
        for r_index, cell in enumerate(source_sheet.col(0)):
            match = re.match(REGEXS['period_monthly'], cell.value)
            if match:
                obs_time = match.group(1) + '-' + match.group(2)
                for c_index, index_code in enumerate(index_codes):
                    obs = Obs(time=obs_time, attributes=[],
                              value=parse_observation_value(
                                  source_sheet.cell(r_index, c_index + 1).value or NAN))
                    series_dict['M.' + index_code.value].observations.append(obs)
        write_json_file(output_dir / 'dataset.json',
                        {**structure_to_dataset_json(dataset_code=structure.id,
                                                     structure=structure,
                                                     lang_candidates=["en"],
                                                     all_series=list(
                                                         series_dict.values())),
                         'updated_at': dataset_source_xls.name.split('_')[0]})
        write_jsonl_file(output_dir / 'series.jsonl',
                         [series_to_series_json(s) for s in series_dict.values()])


def write_json_file(file_path: Path, data):
    """Writes data the JSON way to file_path"""

    with file_path.open('w', encoding='utf-8') as json_fd:
        json.dump(data, json_fd, ensure_ascii=False, indent=2, sort_keys=True)


def write_jsonl_file(file_path: Path, data: Sequence):
    """Writes data the JSON way to file_path"""

    with file_path.open('w', encoding='utf-8') as json_fd:
        for entry in data:
            json.dump(entry, json_fd, ensure_ascii=False, sort_keys=True)
            json_fd.write('\n')


if __name__ == '__main__':
    sys.exit(main())
